{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoders  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder, is an artificial neural network used for learning efficient codings. \n",
    "\n",
    "The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for the purpose of dimensionality reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src =\"imgs/autoencoder.png\" width=\"25%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code loads a local version of mnist dataset \n",
    "import pickle\n",
    "import gzip\n",
    "import sys\n",
    "\n",
    "def load_mnist_local(path):\n",
    "    if path.endswith(\".gz\"):\n",
    "        f = gzip.open(path, 'rb')\n",
    "    else:\n",
    "        f = open(path, 'rb')\n",
    "\n",
    "    if sys.version_info < (3,):\n",
    "        data = pickle.load(f)\n",
    "    else:\n",
    "        data = pickle.load(f, encoding=\"bytes\")\n",
    "\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "path_to_dataset = \"data/mnist.pkl.gz\"\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist_local(path_to_dataset)\n",
    "\n",
    "# squeeze values in [0,1]\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "# flatten images\n",
    "X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
    "X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# based on: https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "# This code uses the functional API of Keras\n",
    "\n",
    "encoding_dim = 32  \n",
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "encoder = Model(input=input_img, output=encoded)\n",
    "\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "\n",
    "#note: X_train, X_train :) \n",
    "autoencoder.fit(X_train, X_train,\n",
    "                nb_epoch=10,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "               validation_split=0.15,\n",
    "               callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "encoded_imgs = encoder.predict(X_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "n = 10 \n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample generation with Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_imgs = np.random.rand(10,32)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "n = 10 \n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # generation\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining encoders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the powerful tools of auto-encoders is using the encoder to generate meaningful representation from the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the encoder to pretrain a classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing using Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> “In God we trust. All others must bring data.” – W. Edwards Deming, statistician"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "### What?\n",
    "Convert words to vectors in a high dimensional space. Each dimension denotes an aspect like gender, type of object / word.\n",
    "\n",
    "\"Word embeddings\" are a family of natural language processing techniques aiming at mapping semantic meaning into a geometric space. This is done by associating a numeric vector to every word in a dictionary, such that the distance (e.g. L2 distance or more commonly cosine distance) between any two vectors would capture part of the semantic relationship between the two associated words. The geometric space formed by these vectors is called an embedding space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why?\n",
    "By converting words to vectors we build relations between words. More similar the words in a dimension, more closer their scores are.\n",
    "\n",
    "### Example\n",
    "_W(green) = (1.2, 0.98, 0.05, ...)_\n",
    "\n",
    "_W(red) = (1.1, 0.2, 0.5, ...)_\n",
    "\n",
    "Here the vector values of _green_ and _red_ are very similar in one dimension because they both are colours. The value for second dimension is very different because red might be depicting something negative in the training data while green is used for positiveness.\n",
    "\n",
    "By vectorizing we are indirectly building different kind of relations between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of `word2vec` using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading blog post from data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = os.path.join(os.path.abspath(os.path.curdir), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male_posts = []\n",
    "female_post = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIRECTORY,\"male_blog_list.txt\"),\"rb\") as male_file:\n",
    "    male_posts= pickle.load(male_file)\n",
    "    \n",
    "with open(os.path.join(DATA_DIRECTORY,\"female_blog_list.txt\"),\"rb\") as female_file:\n",
    "    female_posts = pickle.load(female_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(female_posts))\n",
    "print(len(male_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_male_posts = list(filter(lambda p: len(p) > 0, male_posts))\n",
    "filtered_female_posts = list(filter(lambda p: len(p) > 0, female_posts))\n",
    "posts = filtered_female_posts + filtered_male_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(filtered_female_posts), len(filtered_male_posts), len(posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v = Word2Vec(size=200, min_count=1)\n",
    "w2v.build_vocab(map(lambda x: x.split(), posts[:100]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v.similarity('I', 'My')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(posts[5])\n",
    "w2v.similarity('ring', 'husband')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v.similarity('ring', 'housewife')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v.similarity('women', 'housewife')  # Diversity friendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "\n",
    "The same technique of word2vec is extrapolated to documents. Here, we do everything done in word2vec + we vectorize the documents too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0 for male, 1 for female\n",
    "y_posts = np.concatenate((np.zeros(len(filtered_male_posts)),\n",
    "                          np.ones(len(filtered_female_posts))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(y_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Sentence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train convolutional network for sentiment analysis. Based on\n",
    "\"Convolutional Neural Networks for Sentence Classification\" by Yoon Kim\n",
    "http://arxiv.org/pdf/1408.5882v2.pdf\n",
    "\n",
    "For 'CNN-non-static' gets to 82.1% after 61 epochs with following settings:\n",
    "embedding_dim = 20          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 3\n",
    "dropout_prob = (0.7, 0.8)\n",
    "hidden_dims = 100\n",
    "\n",
    "For 'CNN-rand' gets to 78-79% after 7-8 epochs with following settings:\n",
    "embedding_dim = 20          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 150\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 150\n",
    "\n",
    "For 'CNN-static' gets to 75.4% after 7 epochs with following settings:\n",
    "embedding_dim = 100          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 150\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 150\n",
    "\n",
    "* it turns out that such a small data set as \"Movie reviews with one\n",
    "sentence per review\"  (Pang and Lee, 2005) requires much smaller network\n",
    "than the one introduced in the original article:\n",
    "- embedding dimension is only 20 (instead of 300; 'CNN-static' still requires ~100)\n",
    "- 2 filter sizes (instead of 3)\n",
    "- higher dropout probabilities and\n",
    "- 3 filters per filter size is enough for 'CNN-non-static' (instead of 100)\n",
    "- embedding initialization does not require prebuilt Google Word2Vec data.\n",
    "Training Word2Vec on the same \"Movie reviews\" data set is enough to \n",
    "achieve performance reported in the article (81.6%)\n",
    "\n",
    "** Another distinct difference is slidind MaxPooling window of length=2\n",
    "instead of MaxPooling over whole feature map as in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data_helpers\n",
    "from w2v import train_word2vec\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (Activation, Dense, Dropout, Embedding, \n",
    "                          Flatten, Input, Merge, \n",
    "                          Convolution1D, MaxPooling1D)\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Model Variations. See Kim Yoon's Convolutional Neural Networks for \n",
    "Sentence Classification, Section 3 for detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_variation = 'CNN-rand'  #  CNN-rand | CNN-non-static | CNN-static\n",
    "print('Model variation is %s' % model_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "sequence_length = 56\n",
    "embedding_dim = 20          \n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 150\n",
    "dropout_prob = (0.25, 0.5)\n",
    "hidden_dims = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "val_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word2Vec parameters, see train_word2vec\n",
    "min_word_count = 1  # Minimum word count                        \n",
    "context = 10        # Context window size    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x, y, vocabulary, vocabulary_inv = data_helpers.load_data()\n",
    "\n",
    "if model_variation=='CNN-non-static' or model_variation=='CNN-static':\n",
    "    embedding_weights = train_word2vec(x, vocabulary_inv, \n",
    "                                       embedding_dim, min_word_count, \n",
    "                                       context)\n",
    "    if model_variation=='CNN-static':\n",
    "        x = embedding_weights[0][x]\n",
    "elif model_variation=='CNN-rand':\n",
    "    embedding_weights = None\n",
    "else:\n",
    "    raise ValueError('Unknown model variation')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_in = Input(shape=(sequence_length, embedding_dim))\n",
    "convs = []\n",
    "for fsz in filter_sizes:\n",
    "    conv = Convolution1D(nb_filter=num_filters,\n",
    "                         filter_length=fsz,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(graph_in)\n",
    "    pool = MaxPooling1D(pool_length=2)(conv)\n",
    "    flatten = Flatten()(pool)\n",
    "    convs.append(flatten)\n",
    "    \n",
    "if len(filter_sizes)>1:\n",
    "    out = Merge(mode='concat')(convs)\n",
    "else:\n",
    "    out = convs[0]\n",
    "\n",
    "graph = Model(input=graph_in, output=out)\n",
    "\n",
    "# main sequential model\n",
    "model = Sequential()\n",
    "if not model_variation=='CNN-static':\n",
    "    model.add(Embedding(len(vocabulary), embedding_dim, input_length=sequence_length,\n",
    "                        weights=embedding_weights))\n",
    "model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n",
    "model.add(graph)\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(dropout_prob[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training model\n",
    "# ==================================================\n",
    "model.fit(x_shuffled, y_shuffled, batch_size=batch_size,\n",
    "          nb_epoch=num_epochs, validation_split=val_split, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Another Example\n",
    "\n",
    "Using Keras + [**GloVe**](http://nlp.stanford.edu/projects/glove/) - **Global Vectors for Word Representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word embeddings in a Keras model\n",
    "\n",
    "**Reference:** [https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [deep-learning]",
   "language": "python",
   "name": "Python [deep-learning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
